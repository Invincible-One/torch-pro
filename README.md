# torch-pro

Welcome to **torch-pro**! This repository contains projects aimed at enhancing my advanced PyTorch skills.

## Table of Contents
- [Distributed Parallel Training](#distributed-parallel-training)


## Projects

### Distributed Parallel Training

The **Distributed Parallel Training** section currently contains three mini scripts focusing on DDP:

1. **DDP Overview** ([DDP_overview.py](distributed_parallel_training/DDP_overview.py)): 
   - Implementation of the [PyTorch DDP tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html).
   - Explains the basic mechanisms of DDP.

2. **DDP Multi-GPU** ([DDP_multigpu.py](distributed_parallel_training/DDP_multigpu.py)):
   - Implementation of the [PyTorch Multi-GPU DDP tutorial](https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html).
   - Covers the essentials of multi-GPU training.

3. **DDP with torchrun** ([DDP_torchrun.py](distributed_parallel_training/DDP_torchrun.py)):
   - Implementation of the [PyTorch DDP fault tolerance tutorial](https://pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html).
   - Introduces fault tolerance in DDP.

